---
title: "Workshop"
subtitle: "Data Analysis 1: Core"
toc: true
toc-location: right
---

```{r}
#| include: false
library(tidyverse)
library(kableExtra)
```

# Introduction

## Session overview

In the first part of the workshop I will talk about Project organisation for reproducible analysis and data that has many variables and observations. In the second part of the workshop you will practice getting an overview of such data with summaries and distribution plots, filtering rows and columns.


# Reproducibility

## Why does it matter?

![futureself, CC-BY-NC, by Julen
Colomb](images/future_you.png){fig-alt="Person working at a computer with an offstage person asking 'How is the analysis going?' The person at the computer replies 'Can't understand the date...and the data collector does not answer my emails or calls' Person offstage: 'That's terrible! So cruel! Who did collect the data? I will sack them!' Person at the computer: 'um...I did, 3 years ago.'"
width="400"}

-   Five selfish reasons to work reproducibly [@markowetz2015].
    Alternatively, see the very entertaining
    [talk](https://youtu.be/yVT07Sukv9Q) which covers the the "Duke Scandal".

-   Many high profile cases of work which did not reproduce e.g. Anil
    Potti's work unravelled by @baggerly2009 in the "Duke Scandal"

-   **Will** become standard in Science and publishing e.g OECD Global
    Science Forum Building digital workforce capacity and skills for
    data-intensive science [@oecdglobalscienceforum2020]

## How to achieve reproducibility

-   Scripting

-   Organisation: Project-oriented workflows with file and folder
    structure, naming things

-   Documentation: Comment your code. 



## Project-oriented workflow

-   use folders to organise your work

-   you are aiming for structured, systematic and repeatable.

-   inputs and outputs should be clearly identifiable from structure
    and/or naming

Example

```         
-- stem-cells
   |__stem-cells.Rproj
   |__analysis.R
   |__data-raw
      |__2019-03-21_donor_1.csv
      |__2019-03-21_donor_2.csv
      |__2019-03-21_donor_3.csv
   |__figures
      |__01_volcano_donor_1_vs_donor_2.png
      |__02_volcano_donor_1_vs_donor_3.png
```





# Naming things

-   systematic and consistent

-   informative

-   use same character (e.g., `_` or `-`) for separating parts of the information

-   machine readable

-   human readable

-   play nicely with sorting

I suggest

-   no spaces in names

-   use snake_case or kebab-case rather than CamelCase or dot.case

-   use all lower case 

-   ordering: use left-padded numbers e.g., 01, 02....99 or 001,
    002....999

-   dates [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) format:
    2020-10-16

Example

```{r}
#| echo: false
dir("../week-2/data-raw")
```


# Big data

People argue about what quantity of data constitutes big data. In my view, it is relative to what you are used to and it is not just about the size of the data but also the complexity and the approach to analysis.

You have been used to working with data that has a few explanatory variables, one or two response variables 10 to 100 observations. Typically your approach has been to test one or two specific hypotheses. There is usually a very direct relationship between the experiment you design, the data you collect and the hypotheses you test.

Example: You want to know the effect of an antimicrobial agent on the growth of a bacterium. You design an experiment to measure the growth of the bacterium in the presence and absence of the antimicrobial agent. You collect the data and test the hypothesis that the growth of the bacterium is different in the presence and absence of the antimicrobial agent. You have a dataset with one response variable (growth) and one explanatory variable (presence or absence of the antimicrobial agent) and n observations (replicates of the experiment).


When working with big data we often mean:

-   having many variables (100s or 1000s), many observations or both

-   visualising patterns in the data such as the relationship between variables or observations using "data reduction" techniques such as Principle Components Analysis (PCA)

-   applying hundreds or thousands to tests to identify patterns or relationships amongst the significant results rather than drawing conclusions from individual tests

Example: You want to know the effect of a drug on the expression of genes in a cell. You design an experiment to measure the expression of *all* the genes in the cell in the presence and absence of the drug. You collect the data and test the hypothesis that the expression of any gene is different in the presence and absence of the drug. You then look for patterns in the data such as groups of genes that are co-expressed and the relationship between the expression of genes and the effect of the drug. You have a dataset with many response variables (expression of genes) and one explanatory variable (presence or absence of the drug) and n observations (replicates of the experiment).


# Exercises

## Set up a Project

ðŸŽ¬ Start RStudio from the Start menu

ðŸŽ¬ Make an RStudio project. Be deliberate about where you create it so that it is a good place for you

ðŸŽ¬ Use the Files pane to make a folder for the data. I suggest `data-raw/` 

ðŸŽ¬ Save these data files to the `data-raw/` folder: [biotech-cts.txt](data-raw/biotech-cts.txt), [cell-bio.tsv](data-raw/cell-bio.tsv), [immuno.csv](data-raw/immuno.csv)

ðŸŽ¬ Make a new script called `core-data-analysis.R` to carry out the rest of the work.


## Load packages

We need XXXX packages for this workshop:

-   **`tidyverse`** [@tidyverse]: importing the meta data which is in a text file, working with the data once it is in a dataframe to filter, summarise and plot.


ðŸŽ¬ Load **`tidyverse`**,**`xxxxxx`** and **`xxxxx`**

```{r}

library(tidyverse)
```

## Look at the data!


```{r}
# read the first few lines of data-raw/cell-bio.tsv
read_lines("data-raw/cell-bio.tsv", n_max = 2)
```

```{r}
# read the first few lines of data-raw/biotech-cts.txt
read_lines("data-raw/biotech-cts.txt", n_max = 2)

```
starts with a separator

```{r}
# read the first few lines of data-raw/immuno.csv
read_lines("data-raw/immuno.csv", n_max = 2)

```


## Import the data


```{r}
cell_bio <- read_tsv("data-raw/cell-bio.tsv", n_max = 3)
```

```{r}
cell_bio <- read_tsv("data-raw/cell-bio.tsv")
```

```{r}
biotech <- read_tsv("data-raw/biotech-cts.txt")
```
```{r}
names(biotech)[1] <- "transcript"
```

```{r}
immuno <- read_csv("data-raw/immuno.csv")
```

```{r}
```



```{r}
#|> janitor::clean_names()
```


## Getting an overview



use `View()`

### Summaries

summary
summary stats
group_by


### Visualisation

distributions
boxplots / violin
facet
think about the number of variables and observations!!
ggpairs



## Quality Control

###   filtering rows
a particular value
NA
make things zero


PCA?

```{r}
#| include: false

#---CODING ANSWER---

```



<!-- #---THINKING ANSWER--- -->


You're finished!

# ðŸ¥³ Well Done! ðŸŽ‰



# Independent study following the workshop

[Consolidate](study_after_workshop.qmd)

# The Code file

This contains all the code needed in the workshop even where it is not visible on the webpage.

The `workshop.qmd` file is the file I use to compile the practical. Qmd stands for Quarto markdown. It allows code and ordinary text to be interweaved to produce well-formatted reports including webpages. [View the Qmd in Browser](https://github.com/3mmaRand/R4BABS/blob/main/r4babs4/week-1/workshop.qmd). Coding and thinking answers are marked with `#---CODING ANSWER---` and `#---THINKING ANSWER---`

Pages made with R [@R-core], Quarto [@allaire2022], `knitr` [@knitr], `kableExtra` [@kableExtra]

# References
