---
title: "Workshop"
subtitle: "Data Analysis 1: Core"
toc: true
toc-location: right
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| include: false
library(tidyverse)
library(kableExtra)
```

# Introduction

## Session overview

In the first part of the workshop I will talk about Project organisation
for reproducible analysis and data that has many variables and
observations. In the second part of the workshop you will practice
getting an overview of such data with summaries and distribution plots,
filtering rows and columns.

# Reproducibility

## Why does it matter?

![futureself, CC-BY-NC, by Julen
Colomb](images/future_you.png){fig-alt="Person working at a computer with an offstage person asking 'How is the analysis going?' The person at the computer replies 'Can't understand the date...and the data collector does not answer my emails or calls' Person offstage: 'That's terrible! So cruel! Who did collect the data? I will sack them!' Person at the computer: 'um...I did, 3 years ago.'"
width="400"}

-   Five selfish reasons to work reproducibly [@markowetz2015].
    Alternatively, see the very entertaining
    [talk](https://youtu.be/yVT07Sukv9Q) which covers the the "Duke
    Scandal".

-   Many high profile cases of work which did not reproduce e.g. Anil
    Potti's work unravelled by @baggerly2009 in the "Duke Scandal"

-   **Will** become standard in Science and publishing e.g OECD Global
    Science Forum Building digital workforce capacity and skills for
    data-intensive science [@oecdglobalscienceforum2020]

## How to achieve reproducibility

-   Scripting

-   Organisation: Project-oriented workflows with file and folder
    structure, naming things

-   Documentation: Comment your code.

## Project-oriented workflow

-   use folders to organise your work

-   you are aiming for structured, systematic and repeatable.

-   inputs and outputs should be clearly identifiable from structure
    and/or naming

Example

```         
-- stem-cells
   |__stem-cells.Rproj
   |__analysis.R
   |__data-raw
      |__2019-03-21_donor_1.csv
      |__2019-03-21_donor_2.csv
      |__2019-03-21_donor_3.csv
   |__figures
      |__01_volcano_donor_1_vs_donor_2.png
      |__02_volcano_donor_1_vs_donor_3.png
```

# Naming things

-   systematic and consistent

-   informative

-   use same character (e.g., `_` or `-`) for separating parts of the
    information

-   machine readable

-   human readable

-   play nicely with sorting

I suggest

-   no spaces in names

-   use snake_case or kebab-case rather than CamelCase or dot.case

-   use all lower case

-   ordering: use left-padded numbers e.g., 01, 02....99 or 001,
    002....999

-   dates [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) format:
    2020-10-16

Example

```{r}
#| echo: false
dir("../week-2/data-raw")
```

# Big data

People argue about what quantity of data constitutes big data. In my
view, it is relative to what you are used to and it is not just about
the size of the data but also the complexity and the approach to
analysis.

You have been used to working with data that has a few explanatory
variables, one or two response variables 10 to 100 observations.
Typically your approach has been to test one or two specific hypotheses.
There is usually a very direct relationship between the experiment you
design, the data you collect and the hypotheses you test.

Example: You want to know the effect of an antimicrobial agent on the
growth of a bacterium. You design an experiment to measure the growth of
the bacterium in the presence and absence of the antimicrobial agent.
You collect the data and test the hypothesis that the growth of the
bacterium is different in the presence and absence of the antimicrobial
agent. You have a dataset with one response variable (growth) and one
explanatory variable (presence or absence of the antimicrobial agent)
and n observations (replicates of the experiment).

When working with big data we often mean:

-   having many variables (100s or 1000s), many observations or both

-   visualising patterns in the data such as the relationship between
    variables or observations using "data reduction" techniques such as
    Principle Components Analysis (PCA)

-   applying hundreds or thousands to tests to identify patterns or
    relationships amongst the significant results rather than drawing
    conclusions from individual tests

Example: You want to know the effect of a drug on the expression of
genes in a cell. You design an experiment to measure the expression of
*all* the genes in the cell in the presence and absence of the drug. You
collect the data and test the hypothesis that the expression of any gene
is different in the presence and absence of the drug. You then look for
patterns in the data such as groups of genes that are co-expressed and
the relationship between the expression of genes and the effect of the
drug. You have a dataset with many response variables (expression of
genes) and one explanatory variable (presence or absence of the drug)
and n observations (replicates of the experiment).

# Exercises

## Set up a Project

ðŸŽ¬ Start RStudio from the Start menu

ðŸŽ¬ Make an RStudio project. Be deliberate about where you create it so
that it is a good place for you

ðŸŽ¬ Use the Files pane to make a folder for the data. I suggest
`data-raw/`

ðŸŽ¬ Make a new script called `core-data-analysis.R` to carry out the rest
of the work.

## Load packages

We need XXXX packages for this workshop:

-   **`tidyverse`** [@tidyverse]: importing the meta data which is in a
    text file, working with the data once it is in a dataframe to
    filter, summarise and plot.

ðŸŽ¬ Load **`tidyverse`**,**`xxxxxx`** and **`xxxxx`**

```{r}
library(tidyverse)
```

## Look at the data!

It is almost always useful to take a look at your data (or at least
think about its format) before you attempt to read it in.

We will be working with the following data files in this workshop

-   [biotech-cts.txt](data-raw/biotech-cts.txt). These are RNA-Seq data
    from three replicates for each of two of wheat varieties
    ([S]{.underline}usceptible and [T]{.underline}olerant) grown at two
    potassium conditions ([C]{.underline}ontrol [K]{.underline} and
    [L]{.underline}ow [K]{.underline}). The data are counts of the
    number of reads.

-   [cell-bio.tsv](data-raw/cell-bio.tsv). These are measurements from a
    Livecyte microscope, which performs live cell imaging, tracking
    individual cells, and measurings cell shape and size parameters for
    thousands of cells as they grow and divide. Measurements are taken
    for three replicates from each of two cell lines A and B.

-   [immuno.csv](data-raw/immuno.csv). These are flow cytometry data for
    three treatments with each of two antibodies. The measures are
    Forward scatter, side scatter, red fluorescence and green
    fluorescence.

-   [xlaevis_counts_S20.csv](data-raw/xlaevis_counts_S20.csv). These are
    RNA-Seq data from frogs. There are 2 siblings from each of three
    fertilisations and one sibling was treated with FGF and the other
    was a control. The data are counts of the number of reads.

ðŸŽ¬ Save these data files to the `data-raw/` folder:

ðŸŽ¬ Consider the file extensions. What you think the extensions indicate?

<!-- #---THINKING ANSWER--- -->

<!-- txt usually indicates a plain text file, where the columns are  -->

<!-- often separated by a space but might also be separated by a tab,  -->

<!-- a backslash or forward slash, or some other character.  -->

<!-- Plain text files can be opened in notepad or other similar -->

<!-- editor and still be readable. -->

<!-- csv stands to comma separated values and tsv stands for tab  -->

<!-- separated values. Both of these are plain text format. -->

ðŸŽ¬ Go to the Files pane click on the `xlaevis_counts_S20.csv` file and
choose View File[^1]

[^1]: **Do not** be tempted to import data this way. Unless you are
    careful, your data import will not be scripted or will not be
    scripted correctly. This means your work will not be reproducible.

![RStudio Files
Pane](images/rstudio-filepane-viewfile.png){fig-alt="Rstudio Files pane showing the data files and the View File option that appears when you click on the a particular file"}

Any plain text file will open in the top left pane. Note that this is
NOT importing the data, it's just viewing the file.

ðŸŽ¬ Does it seem to be a csv file?

ðŸŽ¬ Try opening the other files from the Files pane. What happens?

ðŸŽ¬ Open each file in Excel. For the `.txt` and `.tsv` files you will
need to show "All files (*.*)" in the file type dropdown to see them and
you will need to work through the "Text Import Wizard" to open them.

We are opening them each in Excel to see what they look like and to get
a sense of the structure of the data. We are not going to use Excel to
do anything.

ðŸŽ¬ What do you notice? Does it look like this data will be easy to
import and work with?

## Import the data

ðŸŽ¬ Import each of the data files into R:

```{r}
biotech <- read_tsv("data-raw/biotech-cts.txt")
```

```{r}
cell_bio <- read_tsv("data-raw/cell-bio.tsv")
```

```{r}
immuno <- read_csv("data-raw/immuno.csv")
```

```{r}
frogs <- read_csv("data-raw/xlaevis_counts_S20.csv")
```

ðŸŽ¬ Click on each of the dataframes in the Environment pane to see what
they look like. An alternative to clicking is to use the `View()`
function:

```{r}
#| eval: false
View(cell_bio)
```

ðŸŽ¬ In each case, describe what is in the rows and the columns of the
dataframe. Where are the replicates/samples and where are the variables?
Are there any things you'd like to fix/change?

<!-- #---THINKING ANSWER--- -->

ðŸŽ¬ Name of the first column in the `biotech` dataframe:

```{r}
names(biotech)[1] <- "transcript"
```

ðŸŽ¬ Column names in the `cell_bio` dataframe:

```{r}
cell_bio <- janitor::clean_names(cell_bio)
```

## 

## Getting an overview 1

## `summary()`

R's `summary()` function is a quick way to get an overview of datasets.
It gives you a six number summary for every numeric variable (the
minimum, lower quartile, median, mean, upper quartile, and maximum). It
also gives you a count of the number of missing values.

ðŸŽ¬ Use the `summary()` function to get an overview of each dataframe. It
works well for these datasets. What types of dataset might this be less
useful for? Which dataframes and columns have missing values? Which
dataframes and variables seem very skewed? Which dataframes has more
than one character variable?

<!-- #---THINKING ANSWER--- -->

<!-- It is less useful for datasets with very many variables  -->

<!-- (columns) because we aren't able to take that in all  -->

<!-- at once. It is fills up the screen buffer and we can't -->

<!-- see all the output at once. -->

<!-- It is also less useful for datasets with many  -->

<!-- character variables -->

<!-- cell_bio has a perimeter column with 19948 NAs -->

<!-- skew is revealed by the difference between the mean and  -->

<!-- the median and a maximum that is much larger than all  -->

<!-- the other values. The frogs and biotech dataframes have -->

<!-- skewed variables. Skew is typical in RNA-Seq counts. -->

<!-- immuno is the only dataset with more than one character -->

<!-- variable (it has three) -->

## Quality Control

### Filtering rows

The filter function selects/drops a whole row on a condition. The
condition can be based on the value in one or a combination of columns.
We specify what we want to keep with the `filter()` function. This means
you might often want to negate a condition with `!`.

ðŸŽ¬ To remove the rows in `cell_bio` with missing values in the
`perimeter` column:

```{r}
cell_bio <- cell_bio |> filter(!is.na(perimeter))

```

`is.na(perimeter)` returns a logical vector (a vector of `TRUE`s and
`FALSE`s) of the same length as the column. The `!` negates the logical
vector so the `TRUE`s become `FALSE`s and the `FALSE`s become TRUE. This
means that the `filter()` function *keeps* the rows where the perimeter
column is not missing.

ðŸŽ¬ There's actually a tidyverse function that does the same thing as
`filter(!is.na())`

```{r}
cell_bio <- cell_bio |> drop_na(perimeter)

```

Sometimes you might want to work with a subset of data. For example, you
might want to examine just the Media treated cells in the `immuno`
dataset.

ðŸŽ¬ Create a new dataframe called `immuno_media` that contains only the
rows where `treatment` is "MEDIA"

```{r}
immuno_media <- immuno |> 
  filter(treatment == "MEDIA")

```

Or just the Media treated cells with a `FS_Lin` between two values. You
can apply two filters and the `between()` function to achieve this:

ðŸŽ¬ Create a new dataframe called `immuno_media_live` that contains only
the rows where `treatment` is "MEDIA" and `FS_Lin` is between 7500 and
28000:

```{r}
immuno_media_live <- immuno |>
  filter(treatment == "MEDIA") |>
  filter(between(FS_Lin, 7500, 28000))
```

ðŸŽ¬ Now you try. Create a dataframe called `immuno_live` that contains
only the rows where `FS_Lin` is between 7500 and 28000 and `SS_Lin` is
between 15000 and 35000:

```{r}
#| include: false

#---CODING ANSWER---
immuno_live <- immuno |>
  filter(between(FS_Lin, 7500, 28000)) |> 
  filter(between(SS_Lin, 15000, 35000))
```

### Selecting columns

Whilst we can always specify the columns we want to use when working
with data, sometimes we find it less overwhelming to create a new
dataframe with only the columns we want. The `select()` function help us
here.

ðŸŽ¬ Suppose we only want to work with the Susceptible varieties of wheat
in the `biotech` dataframe. We can create a new dataframe with only the
columns we want:

```{r}
biotech_susceptible <- biotech |> 
  select(SCK14_1,
         SCK14_2,
         SCK14_3,
         SLK14_1,
         SLK14_2,
         SLK14_3)

```

In fact, there are a couple of alternatives to this. We can use the
`starts_with()` function to select all the columns that start with a
certain string:

ðŸŽ¬ Select columns starting with `S`

```{r}
biotech_susceptible <- biotech |> 
  select(starts_with("S"))

```

There is an `ends_with()` function too!

ðŸŽ¬ The colon notation allows us to select a range of columns. Select
columns from `SCK14_1` to `SLK14_3`

```{r}
biotech_susceptible <- biotech |> 
  select(SCK14_1:SLK14_3)

```

Note that you need to pay attention to the order of the columns in your
dataframe to use this.

ðŸŽ¬ You can use select and filter together. Try creating a dataframe from
`frogs` which has only the columns from sibling "\_A" and only the rows
where `S20_C_5` is above 20.

```{r}
#| include: false

#---CODING ANSWER---

frogs_subset <- frogs |>   
  filter(S20_C_5 > 20) |> 
  select(ends_with("_A"))

```

### Visualisation

Histograms and density plots are useful for visualising the distribution
of variables when we have many observations. Theya allow you to see features in the data that are not obvious from the summary statistics.

-   A histogram shows the number of values in each range (bin), it is composed of touching bars. How a histogram looks depends on the number of bins used.

-   A density plot shows the proportions on values in a range. The appearance of the distribution is not affected by the number of bins.


ðŸŽ¬ To plot a histogram of `S20_C_5` in the `frogs` dataframe:

```{r}
frogs |> 
  ggplot(aes(x = S20_C_5)) +
  geom_histogram() 


```


This data is very skewed - there are so many low values that we canâ€™t
see the tiny bars for the higher values. Logging the counts is a way to
make the distribution more visible.

ðŸŽ¬ To plot the distribution of log `S20_C_5` in the `frogs` dataframe as a histogram:

```{r}
frogs |> 
  ggplot(aes(x = log10(S20_C_5))) +
  geom_histogram() 


```

ðŸŽ¬ Or a density plot:


```{r}
frogs |> 
  ggplot(aes(x = log10(S20_C_5))) +
  geom_density()

```


Iâ€™ve used base 10 only because it is easy to convert to the original
scale (1 is 10, 2 is 100, 3 is 1000 etc). The warning about rows being
removed is expected - these are the counts of 0 since you canâ€™t log a
value of 0. The peak at zero suggests quite a few counts of 1. We would
expect we would expect the distribution of counts to be roughly log
normal because this is expression of all the genes in the genome[^2].
That peak near the low end is not expected for this distribution and
suggests that these lower counts might be anomalies.

[^2]: This a result of theÂ [Central limit
    theorem](https://en.wikipedia.org/wiki/Central_limit_theorem),one
    consequence of which is that adding together lots of distributions -
    whatever distributions they are - will tend to a normal
    distribution.

The excess number of low counts indicates we might want to create a cut
off for quality control. The removal of low counts is a common
processing step in â€™omic data.


If you have two groups of data, you can compare their distributions on the same plot using the `fill` aesthetic and making the fill transparent with the `alpha` argument.

ðŸŽ¬ To plot the distribution of `area` in the `cell_bio` dataframe as a density plot, with the `clone` variable as the fill:

```{r}
cell_bio |> 
  ggplot(aes(x = area, fill = clone)) +
  geom_density(alpha = 0.5)

```


Boxplots and violin plots are also useful for visualising the
distribution of a variable, especially when there are more than about 5
groups.

ðŸŽ¬ To plot the distribution of `area` in the `cell_bio` dataframe as a boxplot, with the `clone` variable on the x-axis:

```{r}
cell_bio |> 
  ggplot(aes(x = clone, y = area)) +
  geom_boxplot()

```
Suppose you want to see the distribution of many similar variables such as we have in `biotech`. You can apply the group function because the values are in separte columns than than in one column with a grouping variable. However, you can use the `pivot_longer()` function to make the dataframe long and then pipe into ggplot.

ðŸŽ¬ Plot the distribution of the logged counts in the `biotech` dataframe as a boxplot, the samples on the x axis:


```{r}
biotech |> 
  pivot_longer(cols = -transcript,
               names_to = "sample",
               values_to = "count") |> 
  ggplot(aes(x = sample, y = log10(count))) +
  geom_boxplot()

```

The nice thing about the pipe, is that you need not create a new dataframe to do this. You can pipe the output of `pivot_longer()` directly into `ggplot()`. This is very useful when you are exploring the data and want to try out different visualisations.




You're finished!

# ðŸ¥³ Well Done! ðŸŽ‰

# Independent study following the workshop

[Consolidate](study_after_workshop.qmd)

# The Code file

This contains all the code needed in the workshop even where it is not
visible on the webpage.

The `workshop.qmd` file is the file I use to compile the practical. Qmd
stands for Quarto markdown. It allows code and ordinary text to be
interweaved to produce well-formatted reports including webpages. [View
the Qmd in
Browser](https://github.com/3mmaRand/R4BABS/blob/main/r4babs4/week-1/workshop.qmd).
Coding and thinking answers are marked with `#---CODING ANSWER---` and
`#---THINKING ANSWER---`

Pages made with R [@R-core], Quarto [@allaire2022], `knitr` [@knitr],
`kableExtra` [@kableExtra]

# References
